{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e289bcd-b4af-43e7-8dc7-01cfc9375b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark kafka-python delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5434832f-8512-4dcb-bab8-c328017c04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth, lit, udf, floor, concat, lit, window, desc, expr, current_timestamp\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088dd067-6e25-4890-aad1-4a4114798eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\"project2_debs_grand_challenge\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389aefd-cb41-45c2-a3f3-dbbaa8f52c8c",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41374ac2-c8d6-4ad0-8d36-7af0441f1d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"medallion\", \"hack_license\", \"pickup_datetime\", \"dropoff_datetime\",\n",
    "    \"trip_time_in_secs\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "    \"dropoff_longitude\", \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
    "    \"surcharge\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"total_amount\"\n",
    "]\n",
    "\n",
    "\n",
    "df = spark.read.option(\"header\", \"false\").csv(\"data/sorted_data.csv\")\n",
    "df = df.toDF(*columns)\n",
    "df.printSchema()\n",
    "\n",
    "#Take 1GB of original data\n",
    "df_sample = df.sample(withReplacement=False, fraction=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ece2f-6aa2-4bfd-9d73-40d737e7891a",
   "metadata": {},
   "source": [
    "#### Query 0: Data cleaninig and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75bf8722-2a49-4115-9d02-f721404c53af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|DCD53B734DC01B1FB...|9D50A12A43D5078A0...|2013-01-01 00:00:00|2013-01-01 00:04:00|              240|         1.59|      -73.934555|      40.750957|       -73.916328|       40.762241|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|        7.50|\n",
      "|ABA35D2B038DCFBE0...|8DE33EF3E5CB7001F...|2013-01-01 00:03:00|2013-01-01 00:05:00|              120|         1.12|      -73.965721|      40.754436|       -73.956863|       40.766544|         CSH|        5.0|      0.5|    0.5|       0.0|         0.0|        6.00|\n",
      "|D9598D121715B456C...|34887B903219BC5FE...|2013-01-01 00:01:00|2013-01-01 00:06:00|              300|         1.04|      -73.981918|      40.771168|       -73.969978|       40.767078|         CSH|        6.0|      0.5|    0.5|       0.0|         0.0|        7.00|\n",
      "|7CCD0AE562DEC714D...|0F0A8E92E46532047...|2013-01-01 00:02:05|2013-01-01 00:06:07|              242|          1.2|      -73.970940|      40.747574|       -73.982193|       40.740093|         CSH|        5.5|      0.5|    0.5|       0.0|         0.0|        6.50|\n",
      "|4B3E5CA8F5D83D940...|216256AA1ACEBF707...|2013-01-01 00:02:00|2013-01-01 00:07:00|              300|         1.48|      -73.983009|      40.722588|       -73.977844|       40.737911|         CRD|        7.0|      0.5|    0.5|       1.0|         0.0|        9.00|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#converting pickup and dropoff datetimes to to_timestamp\n",
    "df_sample = df_sample.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                     .withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "#Removing null or 0.0 columns and unknown licenses or drivers\n",
    "df_clean = df_sample.filter(\n",
    "    (col(\"medallion\").isNotNull()) & (col(\"medallion\") != \"0\") & (col(\"medallion\") != \"UNKNOWN\") &\n",
    "    (col(\"hack_license\").isNotNull()) & (col(\"hack_license\") != \"0\") & (col(\"hack_license\") != \"UNKNOWN\") &\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &\n",
    "    (col(\"trip_time_in_secs\").isNotNull()) & (col(\"trip_time_in_secs\") != 0) &\n",
    "    (col(\"trip_distance\").isNotNull()) & (col(\"trip_distance\") != 0) &\n",
    "    (col(\"pickup_longitude\").isNotNull()) & (col(\"pickup_longitude\") != 0.0) &\n",
    "    (col(\"pickup_latitude\").isNotNull()) & (col(\"pickup_latitude\") != 0.0) &\n",
    "    (col(\"dropoff_longitude\").isNotNull()) & (col(\"dropoff_longitude\") != 0.0) &\n",
    "    (col(\"dropoff_latitude\").isNotNull()) & (col(\"dropoff_latitude\") != 0.0) &\n",
    "    (col(\"trip_distance\").cast(\"float\") > 0) &\n",
    "    (col(\"fare_amount\").cast(\"float\") > 0)\n",
    ")\n",
    "\n",
    "# Convert relevant columns to appropriate data types for numerical computations\n",
    "df_clean = df_clean.withColumn(\"trip_time_in_secs\", col(\"trip_time_in_secs\").cast(\"int\")) \\\n",
    "                   .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"float\")) \\\n",
    "                   .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"surcharge\", col(\"surcharge\").cast(\"float\")) \\\n",
    "                   .withColumn(\"mta_tax\", col(\"mta_tax\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(\"float\"))\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4306e748-9ed2-43ed-8f39-397565ad6166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13350052"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8cd7c99-6510-4a7d-9449-2bfe2a39ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+-----------+------------+----------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|pickup_year|pickup_month|pickup_day|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+-----------+------------+----------+\n",
      "|DCD53B734DC01B1FB...|9D50A12A43D5078A0...|2013-01-01 00:00:00|2013-01-01 00:04:00|              240|         1.59|      -73.934555|      40.750957|       -73.916328|       40.762241|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|        7.50|       2013|           1|         1|\n",
      "|ABA35D2B038DCFBE0...|8DE33EF3E5CB7001F...|2013-01-01 00:03:00|2013-01-01 00:05:00|              120|         1.12|      -73.965721|      40.754436|       -73.956863|       40.766544|         CSH|        5.0|      0.5|    0.5|       0.0|         0.0|        6.00|       2013|           1|         1|\n",
      "|D9598D121715B456C...|34887B903219BC5FE...|2013-01-01 00:01:00|2013-01-01 00:06:00|              300|         1.04|      -73.981918|      40.771168|       -73.969978|       40.767078|         CSH|        6.0|      0.5|    0.5|       0.0|         0.0|        7.00|       2013|           1|         1|\n",
      "|7CCD0AE562DEC714D...|0F0A8E92E46532047...|2013-01-01 00:02:05|2013-01-01 00:06:07|              242|          1.2|      -73.970940|      40.747574|       -73.982193|       40.740093|         CSH|        5.5|      0.5|    0.5|       0.0|         0.0|        6.50|       2013|           1|         1|\n",
      "|4B3E5CA8F5D83D940...|216256AA1ACEBF707...|2013-01-01 00:02:00|2013-01-01 00:07:00|              300|         1.48|      -73.983009|      40.722588|       -73.977844|       40.737911|         CRD|        7.0|      0.5|    0.5|       1.0|         0.0|        9.00|       2013|           1|         1|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+-----------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the Time Model\n",
    "# Convert the pickup datetime to extract year, month, and day.\n",
    "df_clean = df_clean.withColumn(\"pickup_year\", year(\"pickup_datetime\")) \\\n",
    "                   .withColumn(\"pickup_month\", month(\"pickup_datetime\")) \\\n",
    "                   .withColumn(\"pickup_day\", dayofmonth(\"pickup_datetime\"))\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0da2d4-2dae-449b-ab89-7637e87da193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Cleansed Data with File Partitioning (Parquet format)\n",
    "output_path = \"output/cleansed_taxi_data\"\n",
    "df_clean.write.partitionBy(\"pickup_year\", \"pickup_month\", \"pickup_day\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdd52a-5653-4ce5-91aa-e2cddd50e918",
   "metadata": {},
   "source": [
    "### Query 1: Frequent Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6239c-1070-4d5f-b2a2-380453c279b1",
   "metadata": {},
   "source": [
    "#### Part 1: Finding top 10 most frequent routes during the last 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57137ebc-97f5-40b9-a1e9-cf146c0cd83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dropoff datetime: 2014-01-01 00:42:00\n",
      "+----------+----------+---------------+\n",
      "|start_cell|end_cell  |Number_of_Rides|\n",
      "+----------+----------+---------------+\n",
      "|4076_-7399|4055_-7416|1              |\n",
      "|4075_-7398|4077_-7395|1              |\n",
      "|4064_-7379|4072_-7379|1              |\n",
      "|4078_-7398|4073_-7401|1              |\n",
      "|4071_-7400|4075_-7394|1              |\n",
      "|4075_-7398|4073_-7387|1              |\n",
      "|4075_-7400|4074_-7398|1              |\n",
      "|4071_-7394|4068_-7400|1              |\n",
      "|4073_-7401|4078_-7396|1              |\n",
      "|4072_-7399|4073_-7401|1              |\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute the maximum dropoff datetime in the dataset\n",
    "max_dropoff = df_clean.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "print(\"Max dropoff datetime:\", max_dropoff)\n",
    "\n",
    "# 2. Define a reference time (30 minutes before max_dropoff)\n",
    "ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "# 3. Define grid cell size (here, 0.01 degrees is used as an approximation)\n",
    "cell_size = 0.01\n",
    "\n",
    "# 4. Create grid cell identifiers for pickup (start_cell) and drop-off (end_cell)\n",
    "df_routes = df_clean.withColumn(\n",
    "    \"start_cell\",\n",
    "    concat(\n",
    "        floor(col(\"pickup_latitude\") / lit(cell_size)),\n",
    "        lit(\"_\"),\n",
    "        floor(col(\"pickup_longitude\") / lit(cell_size))\n",
    "    )\n",
    ").withColumn(\n",
    "    \"end_cell\",\n",
    "    concat(\n",
    "        floor(col(\"dropoff_latitude\") / lit(cell_size)),\n",
    "        lit(\"_\"),\n",
    "        floor(col(\"dropoff_longitude\") / lit(cell_size))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Filter the DataFrame to include only trips whose dropoff time is within the last 30 minutes\n",
    "df_last30 = df_routes.filter(col(\"dropoff_datetime\") >= lit(ref_time))\n",
    "\n",
    "# 6. Group by start and end cells and count the number of rides,\n",
    "#    then rename the count column for clarity.\n",
    "df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "\n",
    "# 7. Order the routes by descending ride counts and take the top 10\n",
    "top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "\n",
    "# 8. Show the results\n",
    "top10_routes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301518fd-0e82-496e-a5b4-2e3555a45f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bb34ee3-e4b8-429e-b4c8-ce5964bf2e05",
   "metadata": {},
   "source": [
    "#### Part 2:  Query results must be updated whenever any of the 10 most frequent routes change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b318e276-59e9-4da6-9ecb-6eac10eb2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the cleansed data to a Delta table in a writable directory.\n",
    "df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/taxi_data\")\n",
    "df_stream = spark.readStream.format(\"delta\").load(\"/tmp/delta/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0728fed-dfbe-4fdf-b3b5-f695a8365c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+-----------+------------+----------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|pickup_year|pickup_month|pickup_day|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+-----------+------------+----------+\n",
      "|497948F23936CDE08...|80E833FD673B1E4C0...|2013-02-10 19:20:00|2013-02-10 19:25:00|              300|         1.44|      -73.970955|      40.792850|       -73.961685|       40.802128|         CSH|        6.5|      0.0|    0.5|       0.0|         0.0|        7.00|       2013|           2|        10|\n",
      "|87A5504926726AEF7...|94B39FB827701E0E5...|2013-02-10 19:13:00|2013-02-10 19:25:00|              720|         2.57|      -73.961761|      40.757881|       -73.953209|       40.785477|         CRD|       11.0|      0.0|    0.5|       2.2|         0.0|       13.70|       2013|           2|        10|\n",
      "|8DE28A055AAE70158...|690628DA69F703B0B...|2013-02-10 19:17:00|2013-02-10 19:25:00|              480|         1.55|      -73.966896|      40.766895|       -73.975067|       40.770081|         CSH|        8.0|      0.0|    0.5|       0.0|         0.0|        8.50|       2013|           2|        10|\n",
      "|9397C28CCC23C3CD4...|2E89E94424C6BCAEF...|2013-02-10 19:16:00|2013-02-10 19:25:00|              540|         2.76|      -73.994362|      40.755920|       -74.005371|       40.727108|         CRD|       10.0|      0.0|    0.5|       2.5|         0.0|       13.00|       2013|           2|        10|\n",
      "|B9D9BEFBE80237643...|4333AE8C5F423403B...|2013-02-10 19:16:00|2013-02-10 19:25:00|              540|         1.39|      -74.008408|      40.735416|       -73.994667|       40.750153|         CSH|        7.5|      0.0|    0.5|       0.0|         0.0|        8.00|       2013|           2|        10|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+-----------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the Delta table in batch mode\n",
    "check_df = spark.read.format(\"delta\").load(\"/tmp/delta/taxi_data\")\n",
    "check_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2168f515-fb6e-422e-9ece-703e17dc6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "# This is your foreachBatch function to process each micro-batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Compute the 30-minute window based on the batch’s max dropoff\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "    ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # Compute grid cell IDs for pickup and dropoff using the 500m grid\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"pickup_cell_east\", floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"pickup_cell_south\", floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"start_cell\", concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"dropoff_cell_east\", floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell_south\", floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"end_cell\", concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # Filter out trips that are out-of-bounds (only consider cells 1 to 300)\n",
    "    batch_df = batch_df.filter(\n",
    "        (col(\"pickup_cell_east\").between(1, 300)) &\n",
    "        (col(\"pickup_cell_south\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_east\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_south\").between(1, 300))\n",
    "    )\n",
    "\n",
    "    # Filter for trips with dropoff_datetime >= ref_time (last 30 minutes)\n",
    "    df_last30 = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time))\n",
    "    print(f\"Window filter: dropoff_datetime >= {ref_time}\")\n",
    "    print(\"df_last30 count =\", df_last30.count())\n",
    "    df_last30.show(5)\n",
    "\n",
    "    # Aggregate routes and get top 10 most frequent\n",
    "    df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "    top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "    top10_list = top10_routes.collect()\n",
    "\n",
    "    # Determine a triggering event and compute delay\n",
    "    # Choose the event with the maximum dropoff_datetime as the trigger\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "\n",
    "    # Build the output row\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            route = top10_list[i]\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = route[\"start_cell\"]\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = route[\"end_cell\"]\n",
    "        else:\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id} :\", output_row)\n",
    "    \n",
    "    # Define the output schema explicitly\n",
    "    output_schema = StructType([\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"start_cell_id_1\", StringType(), True),\n",
    "        StructField(\"end_cell_id_1\", StringType(), True),\n",
    "        StructField(\"start_cell_id_2\", StringType(), True),\n",
    "        StructField(\"end_cell_id_2\", StringType(), True),\n",
    "        StructField(\"start_cell_id_3\", StringType(), True),\n",
    "        StructField(\"end_cell_id_3\", StringType(), True),\n",
    "        StructField(\"start_cell_id_4\", StringType(), True),\n",
    "        StructField(\"end_cell_id_4\", StringType(), True),\n",
    "        StructField(\"start_cell_id_5\", StringType(), True),\n",
    "        StructField(\"end_cell_id_5\", StringType(), True),\n",
    "        StructField(\"start_cell_id_6\", StringType(), True),\n",
    "        StructField(\"end_cell_id_6\", StringType(), True),\n",
    "        StructField(\"start_cell_id_7\", StringType(), True),\n",
    "        StructField(\"end_cell_id_7\", StringType(), True),\n",
    "        StructField(\"start_cell_id_8\", StringType(), True),\n",
    "        StructField(\"end_cell_id_8\", StringType(), True),\n",
    "        StructField(\"start_cell_id_9\", StringType(), True),\n",
    "        StructField(\"end_cell_id_9\", StringType(), True),\n",
    "        StructField(\"start_cell_id_10\", StringType(), True),\n",
    "        StructField(\"end_cell_id_10\", StringType(), True),\n",
    "        StructField(\"delay\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create the result DataFrame using the explicit schema\n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    \n",
    "    # Write the result_df as a table (it will create the table if it doesn't exist)\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"frequent_routes\")\n",
    "    \n",
    "    # Optional: Show the result DataFrame\n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# Ensure that your streaming DataFrame has proper types and includes an ingest_time column\n",
    "df_stream = df_stream.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_stream = df_stream.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_stream = df_stream.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Use trigger(once=True) to process existing data exactly one time\n",
    "query = (\n",
    "    df_stream.writeStream\n",
    "    .trigger(once=True)  #This means the query to run just once\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55dc8f5f-a9e1-4f8d-8235-8e0698111b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "|    pickup_datetime|   dropoff_datetime|start_cell_id_1|end_cell_id_1|start_cell_id_2|end_cell_id_2|start_cell_id_3|end_cell_id_3|start_cell_id_4|end_cell_id_4|start_cell_id_5|end_cell_id_5|start_cell_id_6|end_cell_id_6|start_cell_id_7|end_cell_id_7|start_cell_id_8|end_cell_id_8|start_cell_id_9|end_cell_id_9|start_cell_id_10|end_cell_id_10|    delay|\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "|2013-12-31 23:54:08|2014-01-01 00:36:35|        156.160|      152.167|        154.159|      172.160|        158.151|      166.149|        153.163|      171.135|        161.145|      156.162|        157.159|      158.153|        176.157|      157.180|        153.165|      155.158|        153.158|      154.162|         156.161|       154.159|58.904474|\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the result\n",
    "spark.sql(\"SELECT * FROM frequent_routes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb098ff9-4f35-4528-aa91-600b689d921d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "106c042e-b19a-4ba2-88e1-0e3cf06c40a0",
   "metadata": {},
   "source": [
    "### Query 2: Profitable Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb3851-db27-4b1b-b226-6adff9d3f78f",
   "metadata": {},
   "source": [
    "#### Part 1: Report only the 10 most profitable areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a9b558-3cee-4c97-b65f-2fb12f4869ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "def process_batch_query2(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(f\"Batch {batch_id}: Empty batch.\")\n",
    "        return\n",
    "\n",
    "    # Compute reference times based on the batch’s maximum dropoff_datetime\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        print(f\"Batch {batch_id}: max_dropoff is None.\")\n",
    "        return\n",
    "    # For profit: consider trips ending in the last 15 minutes\n",
    "    ref_time_profit = max_dropoff - timedelta(minutes=15)\n",
    "    # For empty taxis: consider taxis whose last dropoff was within the last 30 minutes\n",
    "    ref_time_empty = max_dropoff - timedelta(minutes=30)\n",
    "    print(f\"Batch {batch_id}: ref_time_profit = {ref_time_profit}, ref_time_empty = {ref_time_empty}\")\n",
    "\n",
    "    # Compute profit aggregate per area (using pickup location)\n",
    "    profit_df = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_profit)) \\\n",
    "        .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "        .withColumn(\n",
    "            \"pickup_cell_east\",\n",
    "            floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell\",\n",
    "            concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    profit_agg = profit_df.groupBy(\"pickup_cell\") \\\n",
    "        .agg(F.expr(\"approx_percentile(profit, 0.5) as median_profit\"))\n",
    "    \n",
    "    # Compute empty taxi aggregate per area (using dropoff location)\n",
    "    w = Window.partitionBy(\"medallion\").orderBy(col(\"dropoff_datetime\").desc())\n",
    "    last_dropoff_df = batch_df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "        .filter(col(\"rn\") == 1)\n",
    "    empty_df = last_dropoff_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_empty)) \\\n",
    "        .withColumn(\n",
    "            \"dropoff_cell_east\",\n",
    "            floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell\",\n",
    "            concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    empty_agg = empty_df.groupBy(\"dropoff_cell\") \\\n",
    "        .agg(F.countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "    \n",
    "    # Join the two aggregates on the cell identifier.\n",
    "    area_df = profit_agg.join(empty_agg, profit_agg.pickup_cell == empty_agg.dropoff_cell, \"inner\") \\\n",
    "        .select(profit_agg.pickup_cell.alias(\"cell_id\"), \"median_profit\", \"empty_taxis\") \\\n",
    "        .filter(col(\"empty_taxis\") > 0) \\\n",
    "        .withColumn(\"profitability\", col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "    \n",
    "    top10_areas = area_df.orderBy(col(\"profitability\").desc()).limit(10)\n",
    "    top10_list = top10_areas.collect()\n",
    "    \n",
    "    # Determine a triggering event and compute processing delay.\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    # Use asDict() to safely check for \"ingest_time\"\n",
    "    trigger_row_dict = trigger_row.asDict()\n",
    "    if \"ingest_time\" in trigger_row_dict:\n",
    "        ingest_time = trigger_row_dict[\"ingest_time\"]\n",
    "    else:\n",
    "        ingest_time = trigger_dropoff  # fallback if missing\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "    \n",
    "    # Build the output row.\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            area = top10_list[i]\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = area[\"cell_id\"]\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = str(area[\"empty_taxis\"])\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = area[\"median_profit\"]\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = area[\"profitability\"]\n",
    "        else:\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id}:\", output_row)\n",
    "    \n",
    "    fields = [\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        fields.extend([\n",
    "            StructField(f\"profitable_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"empty_taxies_in_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"median_profit_in_cell_id_{i+1}\", DoubleType(), True),\n",
    "            StructField(f\"profitability_of_cell_{i+1}\", DoubleType(), True)\n",
    "        ])\n",
    "    fields.append(StructField(\"delay\", DoubleType(), True))\n",
    "    output_schema = StructType(fields)\n",
    "    \n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    # Write the result_df as a table\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"most_profitable_areas_result\")\n",
    "    \n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# Ensure your streaming DataFrame has an ingest_time column.\n",
    "df_stream = df_stream.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "query2 = (\n",
    "    df_stream.writeStream\n",
    "    .trigger(once=True) \n",
    "    .foreachBatch(process_batch_query2)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query2.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fea175c-3b2e-4eef-8bf9-37c5d2a0566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "|    pickup_datetime|   dropoff_datetime|profitable_cell_id_1|empty_taxies_in_cell_id_1|median_profit_in_cell_id_1|profitability_of_cell_1|profitable_cell_id_2|empty_taxies_in_cell_id_2|median_profit_in_cell_id_2|profitability_of_cell_2|profitable_cell_id_3|empty_taxies_in_cell_id_3|median_profit_in_cell_id_3|profitability_of_cell_3|profitable_cell_id_4|empty_taxies_in_cell_id_4|median_profit_in_cell_id_4|profitability_of_cell_4|profitable_cell_id_5|empty_taxies_in_cell_id_5|median_profit_in_cell_id_5|profitability_of_cell_5|profitable_cell_id_6|empty_taxies_in_cell_id_6|median_profit_in_cell_id_6|profitability_of_cell_6|profitable_cell_id_7|empty_taxies_in_cell_id_7|median_profit_in_cell_id_7|profitability_of_cell_7|profitable_cell_id_8|empty_taxies_in_cell_id_8|median_profit_in_cell_id_8|profitability_of_cell_8|profitable_cell_id_9|empty_taxies_in_cell_id_9|median_profit_in_cell_id_9|profitability_of_cell_9|profitable_cell_id_10|empty_taxies_in_cell_id_10|median_profit_in_cell_id_10|profitability_of_cell_10|    delay|\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "|2013-12-31 23:54:08|2014-01-01 00:36:35|             159.160|                        1|                      16.0|                   16.0|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                 NULL|                      NULL|                       NULL|                    NULL|45.578305|\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM most_profitable_areas_result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b021f-a897-454a-8b54-4dd4130b2415",
   "metadata": {},
   "source": [
    "#### Part2: Resulting stream of the query provide the 10 most profitable areas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2640527-2a02-4984-9002-1b3f3fb4aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Constants for 250m x 250m grid\n",
    "# The grid’s center of cell 1.1 remains at (41.474937, -74.913585).\n",
    "# For 250m resolution, we use half the previous deltas:\n",
    "new_delta_lat = 0.0045 / 2    # ≈0.00225\n",
    "new_delta_lon = 0.0060 / 2    # ≈0.0030\n",
    "\n",
    "# Define the foreachBatch function for Query 2 Part 2\n",
    "def process_batch_query2_part2(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(f\"Batch {batch_id}: Empty batch.\")\n",
    "        return\n",
    "\n",
    "    # Compute reference times using batch’s maximum dropoff_datetime.\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        print(f\"Batch {batch_id}: max_dropoff is None.\")\n",
    "        return\n",
    "    # For profit computation, consider trips that ended in the last 15 minutes.\n",
    "    ref_time_profit = max_dropoff - timedelta(minutes=15)\n",
    "    # For empty taxis, consider taxis whose last dropoff was within the last 30 minutes.\n",
    "    ref_time_empty = max_dropoff - timedelta(minutes=30)\n",
    "    print(f\"Batch {batch_id}: ref_time_profit = {ref_time_profit}, ref_time_empty = {ref_time_empty}\")\n",
    "\n",
    "    # Compute profit aggregate per area (using pickup location).\n",
    "    # Only consider trips with dropoff_datetime >= ref_time_profit.\n",
    "    profit_df = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_profit)) \\\n",
    "        .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "        .withColumn(\n",
    "            \"pickup_cell_east\",\n",
    "            floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(new_delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(new_delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell\",\n",
    "            concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    profit_agg = profit_df.groupBy(\"pickup_cell\") \\\n",
    "        .agg(F.expr(\"approx_percentile(profit, 0.5) as median_profit\"))\n",
    "    \n",
    "    # Compute empty taxi aggregate per area (using dropoff location).\n",
    "    # For each taxi (medallion), take the latest dropoff event.\n",
    "    w = Window.partitionBy(\"medallion\").orderBy(col(\"dropoff_datetime\").desc())\n",
    "    last_dropoff_df = batch_df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "        .filter(col(\"rn\") == 1)\n",
    "    empty_df = last_dropoff_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_empty)) \\\n",
    "        .withColumn(\n",
    "            \"dropoff_cell_east\",\n",
    "            floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(new_delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(new_delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell\",\n",
    "            concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    empty_agg = empty_df.groupBy(\"dropoff_cell\") \\\n",
    "        .agg(F.countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "    \n",
    "    # Join the aggregates on the cell identifier.\n",
    "    # (We assume the area is defined by the same grid cell for pickup and dropoff.)\n",
    "    area_df = profit_agg.join(empty_agg, profit_agg.pickup_cell == empty_agg.dropoff_cell, \"inner\") \\\n",
    "        .select(profit_agg.pickup_cell.alias(\"cell_id\"), \"median_profit\", \"empty_taxis\") \\\n",
    "        .filter(col(\"empty_taxis\") > 0) \\\n",
    "        .withColumn(\"profitability\", col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "    \n",
    "    # Get the top 10 areas by profitability.\n",
    "    top10_areas = area_df.orderBy(col(\"profitability\").desc()).limit(10)\n",
    "    top10_list = top10_areas.collect()\n",
    "    \n",
    "    # Determine a triggering event and compute processing delay.\n",
    "    # Choose the event with maximum dropoff_datetime as the trigger.\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    # Convert the row to a dictionary for safe field access.\n",
    "    trigger_row_dict = trigger_row.asDict()\n",
    "    # Use ingest_time if available; otherwise, use trigger_dropoff as fallback.\n",
    "    ingest_time = trigger_row_dict[\"ingest_time\"] if \"ingest_time\" in trigger_row_dict else trigger_dropoff\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "    \n",
    "    # Build the output row.\n",
    "    # The required output columns are:\n",
    "    # pickup_datetime, dropoff_datetime, then for each of the 10 areas:\n",
    "    # profitable_cell_id_i, empty_taxies_in_cell_id_i, median_profit_in_cell_id_i, profitability_of_cell_i, and finally delay.\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            area = top10_list[i]\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = area[\"cell_id\"]\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = area[\"empty_taxis\"]  # as integer\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = area[\"median_profit\"]\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = area[\"profitability\"]\n",
    "        else:\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id}:\", output_row)\n",
    "    \n",
    "    # Define the output schema.\n",
    "    out_fields = [\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        out_fields.extend([\n",
    "            StructField(f\"profitable_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"empty_taxies_in_cell_id_{i+1}\", IntegerType(), True),\n",
    "            StructField(f\"median_profit_in_cell_id_{i+1}\", DoubleType(), True),\n",
    "            StructField(f\"profitability_of_cell_{i+1}\", DoubleType(), True)\n",
    "        ])\n",
    "    out_fields.append(StructField(\"delay\", DoubleType(), True))\n",
    "    output_schema = StructType(out_fields)\n",
    "    \n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    # Write the result_df as a table\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"profitable_areas_streaming_result\")\n",
    "    \n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# Ensure your streaming DataFrame (streaming_df) has an ingest_time column.\n",
    "df_stream = df_stream.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Set up the streaming query using foreachBatch.\n",
    "query2_part2 = (\n",
    "    df_stream.writeStream\n",
    "    .trigger(once=True) \n",
    "    .foreachBatch(process_batch_query2_part2)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query2_part2.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "995da771-c899-4e35-8de6-5d4aad053e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "|    pickup_datetime|   dropoff_datetime|profitable_cell_id_1|empty_taxies_in_cell_id_1|median_profit_in_cell_id_1|profitability_of_cell_1|profitable_cell_id_2|empty_taxies_in_cell_id_2|median_profit_in_cell_id_2|profitability_of_cell_2|profitable_cell_id_3|empty_taxies_in_cell_id_3|median_profit_in_cell_id_3|profitability_of_cell_3|profitable_cell_id_4|empty_taxies_in_cell_id_4|median_profit_in_cell_id_4|profitability_of_cell_4|profitable_cell_id_5|empty_taxies_in_cell_id_5|median_profit_in_cell_id_5|profitability_of_cell_5|profitable_cell_id_6|empty_taxies_in_cell_id_6|median_profit_in_cell_id_6|profitability_of_cell_6|profitable_cell_id_7|empty_taxies_in_cell_id_7|median_profit_in_cell_id_7|profitability_of_cell_7|profitable_cell_id_8|empty_taxies_in_cell_id_8|median_profit_in_cell_id_8|profitability_of_cell_8|profitable_cell_id_9|empty_taxies_in_cell_id_9|median_profit_in_cell_id_9|profitability_of_cell_9|profitable_cell_id_10|empty_taxies_in_cell_id_10|median_profit_in_cell_id_10|profitability_of_cell_10|    delay|\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "|2013-12-31 23:54:08|2014-01-01 00:36:35|             317.319|                        1|                      16.0|                   16.0|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                NULL|                     NULL|                      NULL|                   NULL|                 NULL|                      NULL|                       NULL|                    NULL|42.451992|\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM profitable_areas_streaming_result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044972f-5ab4-4c34-a192-8b97958392c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
