{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT 1 - NYC Taxi Data Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze a sample of NYC Taxi trip data using PySpark and Shapely.  \n",
    "We will:\n",
    "\n",
    "1. Load the CSV data\n",
    "2. Filter out outliers (invalid or overly long trips)  \n",
    "3. Enrich the data with borough names using GeoJSON and Shapely  \n",
    "4. Compute several queries:\n",
    "   - **Query 1**: Taxi utilization  \n",
    "   - **Query 2**: Average time to find the next fare (per destination borough)  \n",
    "   - **Query 3**: Number of trips starting and ending in the same borough  \n",
    "   - **Query 4**: Number of trips that start in one borough and end in another  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.11/site-packages (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# from shapely.geometry import shape, Point\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          medallion                      hack_license  \\\n",
      "0  5EE2C4D3BF57BDB455E74B03B89E43A7  E96EF8F6E6122591F9465376043B946D   \n",
      "1  42730E78D8BE872B52598742914DECFF  6016A71F1D29D678E87D36856ED918A7   \n",
      "2  CA6CD9BAED6A85E430F7BFC0BC84ABD0  77FFDF38272A6006517D53EDA14333E2   \n",
      "3  15162141EA7436635C696F5BC023D2D6  CDCB7729DE07243726FF7BB0BD5D06BF   \n",
      "4  025B98A22ED771118FC0EB44A0D3BD9D  7D89374F8E98F30A19F2381EC71A16BA   \n",
      "\n",
      "       pickup_datetime     dropoff_datetime  trip_time_in_secs  trip_distance  \\\n",
      "0  2013-01-01 00:00:09  2013-01-01 00:00:36                 26           0.10   \n",
      "1  2013-01-01 00:01:00  2013-01-01 00:01:00                  0           0.01   \n",
      "2  2013-01-01 00:00:20  2013-01-01 00:01:22                 61           2.20   \n",
      "3  2013-01-01 00:00:14  2013-01-01 00:01:37                 83           0.20   \n",
      "4  2013-01-01 00:00:40  2013-01-01 00:01:40                 60           0.30   \n",
      "\n",
      "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0        -73.992210        40.725124         -73.991646         40.726658   \n",
      "1          0.000000         0.000000           0.000000          0.000000   \n",
      "2        -73.970100        40.768005         -73.969772         40.767834   \n",
      "3        -73.975441        40.749657         -73.977333         40.751991   \n",
      "4        -74.005165        40.720531         -74.003929         40.725655   \n",
      "\n",
      "  payment_type  fare_amount  surcharge mta_tax  tip_amount tolls_amount  total  \n",
      "0          CSH          2.5        0.5  0.50.1         0.0       0.00.1    3.5  \n",
      "1          CSH          2.5        0.5     0.5         0.0          0.0    3.5  \n",
      "2          CSH          3.0        0.5     0.5         0.0          0.0    4.0  \n",
      "3          CSH          3.0        0.5     0.5         0.0          0.0    4.0  \n",
      "4          CSH          3.0        0.5     0.5         0.0          0.0    4.0  \n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"medallion\",\n",
    "    \"hack_license\",\n",
    "    \"pickup_datetime\",\n",
    "    \"dropoff_datetime\",\n",
    "    \"trip_time_in_secs\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"payment_type\",\n",
    "    \"fare_amount\",\n",
    "    \"surcharge\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"total\"  # The 17th column\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/sample.csv\",\n",
    "    header=None,\n",
    "    names=columns,\n",
    "    nrows=10_000\n",
    ")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished sending data to Kafka!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Kafka producer\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:9092')  # or \"localhost:9092\" if not in Docker\n",
    "\n",
    "# Convert each row to JSON and send to Kafka topic \"nyc-taxi-raw\"\n",
    "for _, row in df.iterrows():\n",
    "    row_dict = row.to_dict()  # e.g. {\"medallion\": \"5EE2C4...\", \"hack_license\": \"E96EF8F6E6...\", ...}\n",
    "    producer.send(\"nyc-taxi-clean\", json.dumps(row_dict).encode())\n",
    "\n",
    "producer.flush()\n",
    "print(\"✅ Finished sending data to Kafka!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, year, month, dayofmonth\n",
    "\n",
    "# 1. Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", StringType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", StringType(), True),\n",
    "    StructField(\"total\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Start Spark\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"debs_grand_challenge\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "        \"org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.1\"\n",
    "    ) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kafka_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"nyc-taxi-clean\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(key=None, value=bytearray(b'{\"medallion\": \"5EE2C4D3BF57BDB455E74B03B89E43A7\", \"hack_license\": \"E96EF8F6E6122591F9465376043B946D\", \"pickup_datetime\": \"2013-01-01 00:00:09\", \"dropoff_datetime\": \"2013-01-01 00:00:36\", \"trip_time_in_secs\": 26, \"trip_distance\": 0.1, \"pickup_longitude\": -73.99221, \"pickup_latitude\": 40.725124, \"dropoff_longitude\": -73.991646, \"dropoff_latitude\": 40.726658, \"payment_type\": \"CSH\", \"fare_amount\": 2.5, \"surcharge\": 0.5, \"mta_tax\": \"0.50.1\", \"tip_amount\": 0.0, \"tolls_amount\": \"0.00.1\", \"total\": 3.5}'), topic='nyc-taxi-clean', partition=0, offset=0, timestamp=datetime.datetime(2025, 3, 22, 14, 19, 54, 398000), timestampType=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|raw_string                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"medallion\": \"5EE2C4D3BF57BDB455E74B03B89E43A7\", \"hack_license\": \"E96EF8F6E6122591F9465376043B946D\", \"pickup_datetime\": \"2013-01-01 00:00:09\", \"dropoff_datetime\": \"2013-01-01 00:00:36\", \"trip_time_in_secs\": 26, \"trip_distance\": 0.1, \"pickup_longitude\": -73.99221, \"pickup_latitude\": 40.725124, \"dropoff_longitude\": -73.991646, \"dropoff_latitude\": 40.726658, \"payment_type\": \"CSH\", \"fare_amount\": 2.5, \"surcharge\": 0.5, \"mta_tax\": \"0.50.1\", \"tip_amount\": 0.0, \"tolls_amount\": \"0.00.1\", \"total\": 3.5}|\n",
      "|{\"medallion\": \"42730E78D8BE872B52598742914DECFF\", \"hack_license\": \"6016A71F1D29D678E87D36856ED918A7\", \"pickup_datetime\": \"2013-01-01 00:01:00\", \"dropoff_datetime\": \"2013-01-01 00:01:00\", \"trip_time_in_secs\": 0, \"trip_distance\": 0.01, \"pickup_longitude\": 0.0, \"pickup_latitude\": 0.0, \"dropoff_longitude\": 0.0, \"dropoff_latitude\": 0.0, \"payment_type\": \"CSH\", \"fare_amount\": 2.5, \"surcharge\": 0.5, \"mta_tax\": \"0.5\", \"tip_amount\": 0.0, \"tolls_amount\": \"0.0\", \"total\": 3.5}                               |\n",
      "|{\"medallion\": \"CA6CD9BAED6A85E430F7BFC0BC84ABD0\", \"hack_license\": \"77FFDF38272A6006517D53EDA14333E2\", \"pickup_datetime\": \"2013-01-01 00:00:20\", \"dropoff_datetime\": \"2013-01-01 00:01:22\", \"trip_time_in_secs\": 61, \"trip_distance\": 2.2, \"pickup_longitude\": -73.9701, \"pickup_latitude\": 40.768005, \"dropoff_longitude\": -73.969772, \"dropoff_latitude\": 40.767834, \"payment_type\": \"CSH\", \"fare_amount\": 3.0, \"surcharge\": 0.5, \"mta_tax\": \"0.5\", \"tip_amount\": 0.0, \"tolls_amount\": \"0.0\", \"total\": 4.0}       |\n",
      "|{\"medallion\": \"15162141EA7436635C696F5BC023D2D6\", \"hack_license\": \"CDCB7729DE07243726FF7BB0BD5D06BF\", \"pickup_datetime\": \"2013-01-01 00:00:14\", \"dropoff_datetime\": \"2013-01-01 00:01:37\", \"trip_time_in_secs\": 83, \"trip_distance\": 0.2, \"pickup_longitude\": -73.975441, \"pickup_latitude\": 40.749657, \"dropoff_longitude\": -73.977333, \"dropoff_latitude\": 40.751991, \"payment_type\": \"CSH\", \"fare_amount\": 3.0, \"surcharge\": 0.5, \"mta_tax\": \"0.5\", \"tip_amount\": 0.0, \"tolls_amount\": \"0.0\", \"total\": 4.0}     |\n",
      "|{\"medallion\": \"025B98A22ED771118FC0EB44A0D3BD9D\", \"hack_license\": \"7D89374F8E98F30A19F2381EC71A16BA\", \"pickup_datetime\": \"2013-01-01 00:00:40\", \"dropoff_datetime\": \"2013-01-01 00:01:40\", \"trip_time_in_secs\": 60, \"trip_distance\": 0.3, \"pickup_longitude\": -74.005165, \"pickup_latitude\": 40.720531, \"dropoff_longitude\": -74.003929, \"dropoff_latitude\": 40.725655, \"payment_type\": \"CSH\", \"fare_amount\": 3.0, \"surcharge\": 0.5, \"mta_tax\": \"0.5\", \"tip_amount\": 0.0, \"tolls_amount\": \"0.0\", \"total\": 4.0}     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_str_df = kafka_df.selectExpr(\"CAST(value AS STRING) AS raw_string\")\n",
    "\n",
    "raw_str_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|5EE2C4D3BF57BDB455E74B03B89E43A7|E96EF8F6E6122591F9465376043B946D|2013-01-01 00:00:09|2013-01-01 00:00:36|26               |0.1          |-73.99221       |40.725124      |-73.991646       |40.726658       |CSH         |2.5        |0.5      |0.50.1 |0.0       |0.00.1      |3.5  |\n",
      "|42730E78D8BE872B52598742914DECFF|6016A71F1D29D678E87D36856ED918A7|2013-01-01 00:01:00|2013-01-01 00:01:00|0                |0.01         |0.0             |0.0            |0.0              |0.0             |CSH         |2.5        |0.5      |0.5    |0.0       |0.0         |3.5  |\n",
      "|CA6CD9BAED6A85E430F7BFC0BC84ABD0|77FFDF38272A6006517D53EDA14333E2|2013-01-01 00:00:20|2013-01-01 00:01:22|61               |2.2          |-73.9701        |40.768005      |-73.969772       |40.767834       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |\n",
      "|15162141EA7436635C696F5BC023D2D6|CDCB7729DE07243726FF7BB0BD5D06BF|2013-01-01 00:00:14|2013-01-01 00:01:37|83               |0.2          |-73.975441      |40.749657      |-73.977333       |40.751991       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |\n",
      "|025B98A22ED771118FC0EB44A0D3BD9D|7D89374F8E98F30A19F2381EC71A16BA|2013-01-01 00:00:40|2013-01-01 00:01:40|60               |0.3          |-74.005165      |40.720531      |-74.003929       |40.725655       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "parsed_df = raw_str_df.select(\n",
    "    from_json(col(\"raw_string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "parsed_df.show(5, truncate=False)\n",
    "parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean = parsed_df.dropna()\n",
    "\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"pickup_longitude\") != 0.0) &\n",
    "    (col(\"pickup_latitude\") != 0.0) &\n",
    "    (col(\"dropoff_longitude\") != 0.0) &\n",
    "    (col(\"dropoff_latitude\") != 0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|5EE2C4D3BF57BDB45...|E96EF8F6E6122591F...|2013-01-01 00:00:09|2013-01-01 00:00:36|               26|          0.1|       -73.99221|      40.725124|       -73.991646|       40.726658|         CSH|        2.5|      0.5| 0.50.1|       0.0|      0.00.1|  3.5|\n",
      "|CA6CD9BAED6A85E43...|77FFDF38272A60065...|2013-01-01 00:00:20|2013-01-01 00:01:22|               61|          2.2|        -73.9701|      40.768005|       -73.969772|       40.767834|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|  4.0|\n",
      "|15162141EA7436635...|CDCB7729DE0724372...|2013-01-01 00:00:14|2013-01-01 00:01:37|               83|          0.2|      -73.975441|      40.749657|       -73.977333|       40.751991|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|  4.0|\n",
      "|025B98A22ED771118...|7D89374F8E98F30A1...|2013-01-01 00:00:40|2013-01-01 00:01:40|               60|          0.3|      -74.005165|      40.720531|       -74.003929|       40.725655|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|  4.0|\n",
      "|07290D3599E7A0D62...|E7750A37CAB07D0DF...|2013-01-01 00:00:00|2013-01-01 00:02:00|              120|         0.44|      -73.956528|      40.716976|        -73.96244|       40.715008|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|  4.5|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29175"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 6. Add time columns\n",
    "df_time = df_clean.withColumn(\n",
    "    \"pickup_ts\", to_timestamp(\"pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ").withColumn(\"year\", year(\"pickup_ts\")) \\\n",
    " .withColumn(\"month\", month(\"pickup_ts\")) \\\n",
    " .withColumn(\"day\", dayofmonth(\"pickup_ts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done reading from Kafka, cleaning, and writing partitioned data!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Partitioned Parquet Output\n",
    "df_time.write \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"data/kafka_cleaned_partitioned\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"✅ Done reading from Kafka, cleaning, and writing partitioned data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o371.parquet.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2702)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_check \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/kafka_cleaned_partitioned/year=2013/month=1/day=1/part-00000-3b5227ff-fe95-41b2-8463-905a3756fde3.c000.snappy.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_check\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m df_check\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o371.parquet.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2702)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "df_check = spark.read.parquet(\"data/kafka_cleaned_partitioned/year=2013/month=1/day=1/part-00000-3b5227ff-fe95-41b2-8463-905a3756fde3.c000.snappy.parquet\")\n",
    "df_check.show(10, truncate=False)\n",
    "df_check.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
