{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT 2: THE DEBS CHALLENGE 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /usr/local/spark/python (from delta-spark) (3.5.3)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.20.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python delta-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_json, struct, from_json, to_timestamp, year, month, dayofmonth, regexp_extract, floor, concat, expr, current_timestamp\n",
    "from pyspark.sql.functions import udf, current_timestamp, expr, col, count, to_timestamp, year, month, dayofmonth, lit, max as max_\n",
    "from pyspark.sql import DataFrame\n",
    "from datetime import timedelta, datetime\n",
    "from delta import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", StringType(), True),      \n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", StringType(), True), \n",
    "    StructField(\"total\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 0: Data cleaning and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"debs_grand_challenge\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "        \"org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.1\"\n",
    "    ) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished sending data to Kafka (nyc-taxi-clean)!\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"data/sample.csv\")      \n",
    "\n",
    "df_to_kafka = df_spark.select(to_json(struct(\"*\")).alias(\"value\"))\n",
    "\n",
    "df_to_kafka.write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"nyc-taxi-clean\") \\\n",
    "    .save()\n",
    "\n",
    "print(\" Finished sending data to Kafka (nyc-taxi-clean)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"nyc-taxi-clean\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Kafka binary \"value\" to string\n",
    "raw_str_df = kafka_df.selectExpr(\"CAST(value AS STRING) AS raw_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed DataFrame from Kafka (preview):\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|7B75A4AB3E535F48D4C0429851C4FC0A|032A6DB9395CDD22DC6BAEDD78E9B587|2013-01-02 06:20:06|2013-01-02 06:25:44|338              |1.2          |-73.972343      |40.749863      |-73.982697       |40.760487       |CRD         |6.5        |0.0      |0.5    |1.0       |0.0         |8.0  |\n",
      "|574DF8C3233C672473DC0F1D82D8B302|FD433941F709A5F13EF7856B93A1A509|2013-01-02 23:48:49|2013-01-02 23:58:37|587              |2.3          |-73.948547      |40.778046      |-73.979546       |40.781998       |CRD         |10.0       |0.5      |0.5    |2.2       |0.0         |13.2 |\n",
      "|7C1722E153880F1AD29222C1807EBE3B|D0A83FE92D1D170C20BA1A1EA7713027|2013-01-02 17:17:53|2013-01-02 17:22:04|251              |0.9          |-73.990959      |40.745464      |-74.001884       |40.740215       |CRD         |5.0        |1.0      |0.5    |1.3       |0.0         |7.8  |\n",
      "|4EE20C42C8FDC2AE756897934A8DC863|0A5B72A81E3F5EA0FCEF77CA201D4F2E|2013-01-02 13:47:00|2013-01-02 13:51:00|240              |1.06         |-73.962601      |40.766869      |-73.952179       |40.777882       |CRD         |5.5        |0.0      |0.5    |1.1       |0.0         |7.1  |\n",
      "|B88DDF3A57F62D25F266841B7042985E|96DD49BB84FB62279DBC7AE8047AC36B|2013-01-03 08:25:28|2013-01-03 08:43:51|1102             |3.4          |-73.950714      |40.779213      |-73.994522       |40.766235       |CRD         |15.0       |0.0      |0.5    |2.0       |0.0         |17.5 |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse JSON with the same schema\n",
    "parsed_df = raw_str_df.select(from_json(col(\"raw_string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "print(\"Parsed DataFrame from Kafka (preview):\")\n",
    "parsed_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|7B75A4AB3E535F48D...|032A6DB9395CDD22D...|2013-01-02 06:20:06|2013-01-02 06:25:44|              338|          1.2|      -73.972343|      40.749863|       -73.982697|       40.760487|         CRD|        6.5|      0.0|    0.5|       1.0|         0.0|  8.0|\n",
      "|574DF8C3233C67247...|FD433941F709A5F13...|2013-01-02 23:48:49|2013-01-02 23:58:37|              587|          2.3|      -73.948547|      40.778046|       -73.979546|       40.781998|         CRD|       10.0|      0.5|    0.5|       2.2|         0.0| 13.2|\n",
      "|7C1722E153880F1AD...|D0A83FE92D1D170C2...|2013-01-02 17:17:53|2013-01-02 17:22:04|              251|          0.9|      -73.990959|      40.745464|       -74.001884|       40.740215|         CRD|        5.0|      1.0|    0.5|       1.3|         0.0|  7.8|\n",
      "|4EE20C42C8FDC2AE7...|0A5B72A81E3F5EA0F...|2013-01-02 13:47:00|2013-01-02 13:51:00|              240|         1.06|      -73.962601|      40.766869|       -73.952179|       40.777882|         CRD|        5.5|      0.0|    0.5|       1.1|         0.0|  7.1|\n",
      "|B88DDF3A57F62D25F...|96DD49BB84FB62279...|2013-01-03 08:25:28|2013-01-03 08:43:51|             1102|          3.4|      -73.950714|      40.779213|       -73.994522|       40.766235|         CRD|       15.0|      0.0|    0.5|       2.0|         0.0| 17.5|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing the null values in the data\n",
    "df_clean = parsed_df.dropna()\n",
    "\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"pickup_longitude\") != 0.0) &\n",
    "    (col(\"pickup_latitude\") != 0.0) &\n",
    "    (col(\"dropoff_longitude\") != 0.0) &\n",
    "    (col(\"dropoff_latitude\") != 0.0)\n",
    ")\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975021"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Add time columns\n",
    "df_time = df_clean.withColumn(\n",
    "    \"pickup_ts\", to_timestamp(\"pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"dropoff_ts\", to_timestamp(\"dropoff_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ").withColumn(\"year\", year(\"pickup_ts\")) \\\n",
    " .withColumn(\"month\", month(\"pickup_ts\")) \\\n",
    " .withColumn(\"day\", dayofmonth(\"pickup_ts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done reading from Kafka, cleaning, and writing partitioned data!\n"
     ]
    }
   ],
   "source": [
    "# 7. Partitioned Parquet Output\n",
    "df_time.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"data/kafka_cleaned_partitioned\")\n",
    "\n",
    "#spark.stop()\n",
    "print(\" Done reading from Kafka, cleaning, and writing partitioned data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+-------------------+-------------------+---+----+-----+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|pickup_ts          |dropoff_ts         |day|year|month|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+-------------------+-------------------+---+----+-----+\n",
      "|4EE20C42C8FDC2AE756897934A8DC863|0A5B72A81E3F5EA0FCEF77CA201D4F2E|2013-01-02 13:47:00|2013-01-02 13:51:00|240              |1.06         |-73.962601      |40.766869      |-73.952179       |40.777882       |CRD         |5.5        |0.0      |0.5    |1.1       |0.0         |7.1  |2013-01-02 13:47:00|2013-01-02 13:51:00|2  |2013|1    |\n",
      "|E8727E734249B6D9D6203558AA9FF397|DD239FA4C23EFE5C286EA67D437F93C4|2013-01-02 19:09:42|2013-01-02 19:14:22|280              |0.7          |-73.947411      |40.78347       |-73.955635       |40.77935        |CSH         |5.0        |1.0      |0.5    |0.0       |0.0         |6.5  |2013-01-02 19:09:42|2013-01-02 19:14:22|2  |2013|1    |\n",
      "|D8DFDC9BFAE1C789D65BE2074FAE987A|37E8DEB50F7ACC4C76C871B97286D7E2|2013-01-02 19:56:00|2013-01-02 20:09:00|780              |2.92         |-73.982414      |40.771492      |-73.964973       |40.775368       |CSH         |12.0       |1.0      |0.5    |0.0       |0.0         |13.5 |2013-01-02 19:56:00|2013-01-02 20:09:00|2  |2013|1    |\n",
      "|F0FBEA57D20A0E9AFEC0F0ED4F0952B4|7B96988415AF4DC5055646CE705BCE7E|2013-01-02 19:03:02|2013-01-02 19:14:22|679              |1.5          |-73.966484      |40.768848      |-73.980064       |40.778839       |CRD         |9.0        |1.0      |0.5    |1.5       |0.0         |12.0 |2013-01-02 19:03:02|2013-01-02 19:14:22|2  |2013|1    |\n",
      "|E3338C03E912CB4C81E27499F45CE5BC|53E8732A1E652981014EDE7C90285DED|2013-01-02 10:05:12|2013-01-02 10:13:41|509              |1.7          |-73.980301      |40.78231       |-73.973289       |40.764076       |CSH         |8.5        |0.0      |0.5    |0.0       |0.0         |9.0  |2013-01-02 10:05:12|2013-01-02 10:13:41|2  |2013|1    |\n",
      "|09D564D2827B0E2E8654057E0F724283|E7753ABDB4180BD5E51C9874574F0134|2013-01-02 18:35:16|2013-01-02 19:14:23|2346             |17.4         |-73.962708      |40.756031      |-73.776291       |40.645844       |CRD         |52.0       |0.0      |0.5    |11.0      |4.8         |68.3 |2013-01-02 18:35:16|2013-01-02 19:14:23|2  |2013|1    |\n",
      "|5D1EFEFC3E43D445885529F69114FA7E|70BC0C82075F3D5E41E42CF2944DA955|2013-01-01 16:54:20|2013-01-01 17:01:17|417              |1.8          |-73.968086      |40.765381      |-73.985115       |40.744621       |CRD         |8.0        |0.0      |0.5    |2.0       |0.0         |10.5 |2013-01-01 16:54:20|2013-01-01 17:01:17|1  |2013|1    |\n",
      "|1C0621B8B81EEC5FCCF2B606E90004B0|AA38851431A0EC5925B0A61EE7891837|2013-01-02 19:00:03|2013-01-02 19:14:23|860              |5.7          |-73.955734      |40.763992      |-73.942665       |40.823181       |CSH         |18.5       |1.0      |0.5    |0.0       |0.0         |20.0 |2013-01-02 19:00:03|2013-01-02 19:14:23|2  |2013|1    |\n",
      "|FED1D8953D9019BD97834EC4AE11C4FB|D08BBF99C5ED927E1C4C22939E8510FD|2013-01-01 06:16:37|2013-01-01 06:22:12|334              |1.0          |-73.983002      |40.730686      |-73.982933       |40.72105        |CSH         |6.0        |0.0      |0.5    |0.0       |0.0         |6.5  |2013-01-01 06:16:37|2013-01-01 06:22:12|1  |2013|1    |\n",
      "|4A7EE5BF5B482E3CA7D84110E78E8104|77EE14A72DAE44C267013CE855BB651F|2013-01-02 19:07:35|2013-01-02 19:14:23|408              |1.1          |-73.975182      |40.760509      |-73.962311       |40.764614       |CRD         |6.5        |1.0      |0.5    |1.5       |0.0         |9.5  |2013-01-02 19:07:35|2013-01-02 19:14:23|2  |2013|1    |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+-------------------+-------------------+---+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now create a new session\n",
    "new_spark = SparkSession.builder.appName(\"CheckParquet\").getOrCreate()\n",
    "\n",
    "df_check = new_spark.read.parquet(\"data/kafka_cleaned_partitioned/\")\n",
    "df_check.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1:  Frequent Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Finding top 10 most frequent routes during the last 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_cell(lat, lon):\n",
    "    # This simple grid resolution multiplies coordinates by 100.\n",
    "    return f\"{int(lat * 100)}_{int(lon * 100)}\"\n",
    "\n",
    "get_grid_cell_udf = udf(get_grid_cell, StringType())\n",
    "\n",
    "# Add new columns for the starting and ending grid cell IDs.\n",
    "df_with_cells = df_time.withColumn(\"start_cell\", \n",
    "                                   get_grid_cell_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))\n",
    "                                  ).withColumn(\"end_cell\", \n",
    "                                   get_grid_cell_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))\n",
    "                                  )\n",
    "\n",
    "# Determine the maximum dropoff timestamp in your data.\n",
    "max_dropoff_row = df_with_cells.select(max_(\"dropoff_ts\").alias(\"max_dropoff\")).first()\n",
    "max_dropoff_ts = max_dropoff_row[\"max_dropoff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Frequent Routes in the Last 30 Minutes (relative to max dropoff time):\n",
      "+----------+----------+---------------+\n",
      "|start_cell|end_cell  |Number_of_Rides|\n",
      "+----------+----------+---------------+\n",
      "|4075_-7397|4074_-7398|352            |\n",
      "|4075_-7397|4075_-7397|308            |\n",
      "|4075_-7397|4075_-7398|300            |\n",
      "|4076_-7397|4075_-7398|300            |\n",
      "|4076_-7396|4075_-7397|300            |\n",
      "|4076_-7397|4075_-7397|288            |\n",
      "|4074_-7398|4075_-7398|276            |\n",
      "|4075_-7397|4076_-7397|276            |\n",
      "|4075_-7398|4075_-7398|268            |\n",
      "|4074_-7398|4075_-7397|268            |\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate a threshold that is 30 minutes before the max dropoff time.\n",
    "time_threshold = max_dropoff_ts - datetime.timedelta(minutes=30)\n",
    "    \n",
    "# Filter the data to include only rides completed within these 30 minutes.\n",
    "df_recent = df_with_cells.filter(col(\"dropoff_ts\") >= lit(time_threshold))\n",
    "\n",
    "# Group by starting and ending grid cells and count the rides.\n",
    "df_routes = df_recent.groupBy(\"start_cell\", \"end_cell\").agg(count(\"*\").alias(\"Number_of_Rides\"))\n",
    "    \n",
    "# Select the top 10 most frequent routes.\n",
    "top_routes = df_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "\n",
    "print(\"Top 10 Frequent Routes in the Last 30 Minutes (relative to max dropoff time):\")\n",
    "top_routes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2:  Query results must be updated whenever any of the 10 most frequent routes change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"nyc-taxi-clean\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- ingest_time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary 'value' from Kafka to a string and parse the JSON payload.\n",
    "streaming_df = kafka_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Add an 'ingest_time' column to mark when the event was read.\n",
    "streaming_df = streaming_df.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Optional: Print the schema to verify\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "# This is your foreachBatch function to process each micro-batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # PART A: Compute the 30-minute window based on the batch’s max dropoff\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "    ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # PART B: Compute grid cell IDs for pickup and dropoff using the 500m grid\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"pickup_cell_east\", floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"pickup_cell_south\", floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"start_cell\", concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"dropoff_cell_east\", floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell_south\", floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"end_cell\", concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # Filter out trips that are out-of-bounds (only consider cells 1 to 300)\n",
    "    batch_df = batch_df.filter(\n",
    "        (col(\"pickup_cell_east\").between(1, 300)) &\n",
    "        (col(\"pickup_cell_south\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_east\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_south\").between(1, 300))\n",
    "    )\n",
    "\n",
    "    # PART C: Filter for trips with dropoff_datetime >= ref_time (last 30 minutes)\n",
    "    df_last30 = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time))\n",
    "    print(f\"Window filter: dropoff_datetime >= {ref_time}\")\n",
    "    print(\"df_last30 count =\", df_last30.count())\n",
    "    df_last30.show(5)\n",
    "\n",
    "\n",
    "    # PART D: Aggregate routes and get top 10 most frequent\n",
    "    df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "    top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "    top10_list = top10_routes.collect()\n",
    "\n",
    "    # PART E: Determine a triggering event and compute delay\n",
    "    # Choose the event with the maximum dropoff_datetime as the trigger\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "\n",
    "    # PART F: Build the output row\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            route = top10_list[i]\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = route[\"start_cell\"]\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = route[\"end_cell\"]\n",
    "        else:\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = None\n",
    "\n",
    "    # For demonstration, print the output update\n",
    "    print(f\"Update for batch {batch_id} :\", output_row)\n",
    "    \n",
    "    # Define the output schema explicitly\n",
    "    output_schema = StructType([\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"start_cell_id_1\", StringType(), True),\n",
    "        StructField(\"end_cell_id_1\", StringType(), True),\n",
    "        StructField(\"start_cell_id_2\", StringType(), True),\n",
    "        StructField(\"end_cell_id_2\", StringType(), True),\n",
    "        StructField(\"start_cell_id_3\", StringType(), True),\n",
    "        StructField(\"end_cell_id_3\", StringType(), True),\n",
    "        StructField(\"start_cell_id_4\", StringType(), True),\n",
    "        StructField(\"end_cell_id_4\", StringType(), True),\n",
    "        StructField(\"start_cell_id_5\", StringType(), True),\n",
    "        StructField(\"end_cell_id_5\", StringType(), True),\n",
    "        StructField(\"start_cell_id_6\", StringType(), True),\n",
    "        StructField(\"end_cell_id_6\", StringType(), True),\n",
    "        StructField(\"start_cell_id_7\", StringType(), True),\n",
    "        StructField(\"end_cell_id_7\", StringType(), True),\n",
    "        StructField(\"start_cell_id_8\", StringType(), True),\n",
    "        StructField(\"end_cell_id_8\", StringType(), True),\n",
    "        StructField(\"start_cell_id_9\", StringType(), True),\n",
    "        StructField(\"end_cell_id_9\", StringType(), True),\n",
    "        StructField(\"start_cell_id_10\", StringType(), True),\n",
    "        StructField(\"end_cell_id_10\", StringType(), True),\n",
    "        StructField(\"delay\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create the DataFrame using the explicit schema\n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    # Write the result_df as a table (it will create the table if it doesn't exist)\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"top_ten_routes_output\")\n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# If your taxi data doesn’t already have proper types, ensure you convert the datetime columns\n",
    "streaming_df = streaming_df.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "streaming_df = streaming_df.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "streaming_df = streaming_df.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Use trigger(once=True) to process existing data exactly one time\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .trigger(once=True)                # <--- This forces the query to run just once\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, floor, concat, to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "\n",
    "# Ensure Kafka Stream is working\n",
    "assert kafka_df.isStreaming, \"Kafka DataFrame is not streaming!\"\n",
    "\n",
    "# Convert Kafka binary data into JSON\n",
    "streaming_df = kafka_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Add an ingestion timestamp\n",
    "streaming_df = streaming_df.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Convert datetime fields\n",
    "streaming_df = streaming_df.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "streaming_df = streaming_df.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Define grid constants\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045  \n",
    "delta_lon = 0.0060  \n",
    "\n",
    "# Process micro-batches\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(f\"No data in batch {batch_id}, skipping...\")\n",
    "        return\n",
    "\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "    ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # Compute grid cell IDs\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"pickup_cell_east\", floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"pickup_cell_south\", floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"start_cell\", concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell_east\", floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell_south\", floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"end_cell\", concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # Keep trips in the valid grid range\n",
    "    batch_df = batch_df.filter(\n",
    "        (col(\"pickup_cell_east\").between(1, 300)) &\n",
    "        (col(\"pickup_cell_south\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_east\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_south\").between(1, 300))\n",
    "    )\n",
    "\n",
    "    # Filter last 30 minutes of trips\n",
    "    df_last30 = batch_df.filter(col(\"dropoff_datetime\") >= lit(ref_time))\n",
    "    df_last30.show(5)\n",
    "\n",
    "    # Aggregate most frequent routes\n",
    "    df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\") \\\n",
    "        .count().withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "\n",
    "    # Get top 10 routes\n",
    "    top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "    top10_list = top10_routes.collect()\n",
    "\n",
    "    # Get max dropoff event\n",
    "    trigger_rows = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()\n",
    "    if len(trigger_rows) == 0:\n",
    "        return\n",
    "    trigger_row = trigger_rows[0]\n",
    "\n",
    "    processing_time = datetime.utcnow()\n",
    "    delay = (processing_time - trigger_row[\"ingest_time\"]).total_seconds()\n",
    "\n",
    "    # Create output row\n",
    "    output_row = {\"pickup_datetime\": trigger_row[\"pickup_datetime\"], \"dropoff_datetime\": trigger_row[\"dropoff_datetime\"], \"delay\": delay}\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = top10_list[i][\"start_cell\"]\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = top10_list[i][\"end_cell\"]\n",
    "        else:\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = None\n",
    "\n",
    "    result_df = spark.createDataFrame([output_row])\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"top_ten_routes_output\")\n",
    "\n",
    "# Run streaming query\n",
    "query = streaming_df.writeStream.trigger(once=True).foreachBatch(process_batch).outputMode(\"append\").start()\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `top_ten_routes_output` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [top_ten_routes_output], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM top_ten_routes_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `top_ten_routes_output` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [top_ten_routes_output], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM top_ten_routes_output\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Profitable Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Report only the 10 most profitable areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, floor, concat, lit, current_timestamp, to_timestamp, expr\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, TimestampType, DoubleType, StringType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "def process_batch_query2(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(\"Empty batches\") #for debugging \n",
    "        return\n",
    "\n",
    "    # PART A: Compute reference times based on the batch’s maximum dropoff_datetime\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        print(\"Error At Maxdropoff\") # for debugging\n",
    "        return\n",
    "    # For profit: consider trips ending in the last 15 minutes\n",
    "    ref_time_profit = max_dropoff - timedelta(minutes=15)\n",
    "    # For empty taxis: consider taxis whose last dropoff was within the last 30 minutes\n",
    "    ref_time_empty = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # PART B: Compute profit aggregate per area (using pickup location)\n",
    "    # Only consider trips that ended in the last 15 minutes\n",
    "    profit_df = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_profit)) \\\n",
    "        .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "        .withColumn(\n",
    "            \"pickup_cell_east\",\n",
    "            floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell\",\n",
    "            concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    profit_agg = profit_df.groupBy(\"pickup_cell\") \\\n",
    "        .agg(F.expr(\"approx_percentile(profit, 0.5) as median_profit\"))\n",
    "    \n",
    "    # PART C: Compute empty taxi aggregate per area (using dropoff location)\n",
    "    # For each taxi, take the latest dropoff event and if it occurred in the last 30 minutes, consider it empty.\n",
    "    w = Window.partitionBy(\"medallion\").orderBy(col(\"dropoff_datetime\").desc())\n",
    "    last_dropoff_df = batch_df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "        .filter(col(\"rn\") == 1)\n",
    "    empty_df = last_dropoff_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_empty)) \\\n",
    "        .withColumn(\n",
    "            \"dropoff_cell_east\",\n",
    "            floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell\",\n",
    "            concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    empty_agg = empty_df.groupBy(\"dropoff_cell\") \\\n",
    "        .agg(F.countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "    \n",
    "    # PART D: Join the two aggregates on the cell identifier.\n",
    "    # (Here we assume the area is defined by the same grid cell for pickup and dropoff.)\n",
    "    area_df = profit_agg.join(empty_agg, profit_agg.pickup_cell == empty_agg.dropoff_cell, \"inner\") \\\n",
    "        .select(profit_agg.pickup_cell.alias(\"cell_id\"), \"median_profit\", \"empty_taxis\") \\\n",
    "        .filter(col(\"empty_taxis\") > 0) \\\n",
    "        .withColumn(\"profitability\", col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "    \n",
    "    # Get the top 10 areas by profitability\n",
    "    top10_areas = area_df.orderBy(col(\"profitability\").desc()).limit(10)\n",
    "    top10_list = top10_areas.collect()\n",
    "    \n",
    "    # PART E: Determine a triggering event and compute processing delay.\n",
    "    # Choose the event with the maximum dropoff_datetime as the trigger.\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "    \n",
    "    # PART F: Build the output row.\n",
    "    # The output format is:\n",
    "    # pickup_datetime, dropoff_datetime, and for each of the top 10 areas:\n",
    "    # profitable_cell_id_i, empty_taxies_in_cell_id_i, median_profit_in_cell_id_i, profitability_of_cell_i, then delay.\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    # For each of the top 10 areas, add four columns.\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            area = top10_list[i]\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = area[\"cell_id\"]\n",
    "            # Convert the empty taxi count to string (or leave as numeric) as desired.\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = str(area[\"empty_taxis\"])\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = area[\"median_profit\"]\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = area[\"profitability\"]\n",
    "        else:\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id}:\", output_row)\n",
    "    \n",
    "    # Define the output schema explicitly.\n",
    "    fields = [\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        fields.extend([\n",
    "            StructField(f\"profitable_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"empty_taxies_in_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"median_profit_in_cell_id_{i+1}\", DoubleType(), True),\n",
    "            StructField(f\"profitability_of_cell_{i+1}\", DoubleType(), True)\n",
    "        ])\n",
    "    fields.append(StructField(\"delay\", DoubleType(), True))\n",
    "    output_schema = StructType(fields)\n",
    "    \n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: set up the streaming query (using trigger(once=True) for testing)\n",
    "query2 = (\n",
    "    streaming_df.writeStream\n",
    "    .trigger(once=True)  # For testing; in production you might use a continuous or timed trigger.\n",
    "    .foreachBatch(process_batch_query2)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query2.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
