{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT 1 - NYC Taxi Data Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze a sample of NYC Taxi trip data using PySpark and Shapely.  \n",
    "We will:\n",
    "\n",
    "1. Load the CSV data\n",
    "2. Filter out outliers (invalid or overly long trips)  \n",
    "3. Enrich the data with borough names using GeoJSON and Shapely  \n",
    "4. Compute several queries:\n",
    "   - **Query 1**: Taxi utilization  \n",
    "   - **Query 2**: Average time to find the next fare (per destination borough)  \n",
    "   - **Query 3**: Number of trips starting and ending in the same borough  \n",
    "   - **Query 4**: Number of trips that start in one borough and end in another  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.11/site-packages (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# from shapely.geometry import shape, Point\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", StringType(), True),      # parse as string to avoid \"0.50.1\" errors\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", StringType(), True), # same reason\n",
    "    StructField(\"total\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:47:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"debs_grand_challenge\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "        \"org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.1\"\n",
    "    ) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished sending data to Kafka (nyc-taxi-clean)!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_spark = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"../data/sample.csv\")       \n",
    "\n",
    "print(\"✅ Finished sending data to Kafka (nyc-taxi-clean)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"nyc-taxi-clean\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kafka_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"nyc-taxi-clean\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Kafka binary \"value\" to string\n",
    "raw_str_df = kafka_df.selectExpr(\"CAST(value AS STRING) AS raw_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed DataFrame from Kafka (preview):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:47:09 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|5EE2C4D3BF57BDB455E74B03B89E43A7|E96EF8F6E6122591F9465376043B946D|2013-01-01 00:00:09|2013-01-01 00:00:36|26               |0.1          |-73.99221       |40.725124      |-73.991646       |40.726658       |CSH         |2.5        |0.5      |0.50.1 |0.0       |0.00.1      |3.5  |\n",
      "|42730E78D8BE872B52598742914DECFF|6016A71F1D29D678E87D36856ED918A7|2013-01-01 00:01:00|2013-01-01 00:01:00|0                |0.01         |0.0             |0.0            |0.0              |0.0             |CSH         |2.5        |0.5      |0.5    |0.0       |0.0         |3.5  |\n",
      "|CA6CD9BAED6A85E430F7BFC0BC84ABD0|77FFDF38272A6006517D53EDA14333E2|2013-01-01 00:00:20|2013-01-01 00:01:22|61               |2.2          |-73.9701        |40.768005      |-73.969772       |40.767834       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |\n",
      "|15162141EA7436635C696F5BC023D2D6|CDCB7729DE07243726FF7BB0BD5D06BF|2013-01-01 00:00:14|2013-01-01 00:01:37|83               |0.2          |-73.975441      |40.749657      |-73.977333       |40.751991       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |\n",
      "|025B98A22ED771118FC0EB44A0D3BD9D|7D89374F8E98F30A19F2381EC71A16BA|2013-01-01 00:00:40|2013-01-01 00:01:40|60               |0.3          |-74.005165      |40.720531      |-74.003929       |40.725655       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_json, struct, from_json, to_timestamp, year, month, dayofmonth, regexp_extract\n",
    "\n",
    "# Parse JSON with the same schema\n",
    "parsed_df = raw_str_df.select(from_json(col(\"raw_string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "print(\"Parsed DataFrame from Kafka (preview):\")\n",
    "parsed_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean = parsed_df.dropna()\n",
    "\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"pickup_longitude\") != 0.0) &\n",
    "    (col(\"pickup_latitude\") != 0.0) &\n",
    "    (col(\"dropoff_longitude\") != 0.0) &\n",
    "    (col(\"dropoff_latitude\") != 0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "|5EE2C4D3BF57BDB45...|E96EF8F6E6122591F...|2013-01-01 00:00:09|2013-01-01 00:00:36|               26|          0.1|       -73.99221|      40.725124|       -73.991646|       40.726658|         CSH|        2.5|      0.5| 0.50.1|       0.0|      0.00.1|  3.5|\n",
      "|CA6CD9BAED6A85E43...|77FFDF38272A60065...|2013-01-01 00:00:20|2013-01-01 00:01:22|               61|          2.2|        -73.9701|      40.768005|       -73.969772|       40.767834|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|  4.0|\n",
      "|15162141EA7436635...|CDCB7729DE0724372...|2013-01-01 00:00:14|2013-01-01 00:01:37|               83|          0.2|      -73.975441|      40.749657|       -73.977333|       40.751991|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|  4.0|\n",
      "|025B98A22ED771118...|7D89374F8E98F30A1...|2013-01-01 00:00:40|2013-01-01 00:01:40|               60|          0.3|      -74.005165|      40.720531|       -74.003929|       40.725655|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|  4.0|\n",
      "|07290D3599E7A0D62...|E7750A37CAB07D0DF...|2013-01-01 00:00:00|2013-01-01 00:02:00|              120|         0.44|      -73.956528|      40.716976|        -73.96244|       40.715008|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|  4.5|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:47:09 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:47:09 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29175"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 6. Add time columns\n",
    "df_time = df_clean.withColumn(\n",
    "    \"pickup_ts\", to_timestamp(\"pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ").withColumn(\"year\", year(\"pickup_ts\")) \\\n",
    " .withColumn(\"month\", month(\"pickup_ts\")) \\\n",
    " .withColumn(\"day\", dayofmonth(\"pickup_ts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:47:12 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done reading from Kafka, cleaning, and writing partitioned data!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Partitioned Parquet Output\n",
    "df_time.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"data/kafka_cleaned_partitioned\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"✅ Done reading from Kafka, cleaning, and writing partitioned data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:47:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+-------------------+---+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|pickup_ts          |day|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+-------------------+---+\n",
      "|5EE2C4D3BF57BDB455E74B03B89E43A7|E96EF8F6E6122591F9465376043B946D|2013-01-01 00:00:09|2013-01-01 00:00:36|26               |0.1          |-73.99221       |40.725124      |-73.991646       |40.726658       |CSH         |2.5        |0.5      |0.50.1 |0.0       |0.00.1      |3.5  |2013-01-01 00:00:09|1  |\n",
      "|CA6CD9BAED6A85E430F7BFC0BC84ABD0|77FFDF38272A6006517D53EDA14333E2|2013-01-01 00:00:20|2013-01-01 00:01:22|61               |2.2          |-73.9701        |40.768005      |-73.969772       |40.767834       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |2013-01-01 00:00:20|1  |\n",
      "|15162141EA7436635C696F5BC023D2D6|CDCB7729DE07243726FF7BB0BD5D06BF|2013-01-01 00:00:14|2013-01-01 00:01:37|83               |0.2          |-73.975441      |40.749657      |-73.977333       |40.751991       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |2013-01-01 00:00:14|1  |\n",
      "|025B98A22ED771118FC0EB44A0D3BD9D|7D89374F8E98F30A19F2381EC71A16BA|2013-01-01 00:00:40|2013-01-01 00:01:40|60               |0.3          |-74.005165      |40.720531      |-74.003929       |40.725655       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0  |2013-01-01 00:00:40|1  |\n",
      "|07290D3599E7A0D62097A346EFCC1FB5|E7750A37CAB07D0DFF0AF7E3573AC141|2013-01-01 00:00:00|2013-01-01 00:02:00|120              |0.44         |-73.956528      |40.716976      |-73.96244        |40.715008       |CSH         |3.5        |0.5      |0.5    |0.0       |0.0         |4.5  |2013-01-01 00:00:00|1  |\n",
      "|E79E74C15D90CD93B1564E91E3D64765|145038A0CC99D6982D8001BE668154CA|2013-01-01 00:01:00|2013-01-01 00:02:00|60               |0.45         |-73.95208       |40.790169      |-73.948921       |40.794323       |CSH         |3.5        |0.5      |0.5    |0.0       |0.0         |4.5  |2013-01-01 00:01:00|1  |\n",
      "|FD39403FDE46B6C753DDD6518A4365D7|2B36D07A27BB35D7DF7170C83EEAA196|2013-01-01 00:01:00|2013-01-01 00:02:00|60               |0.34         |-74.003197      |40.708313      |-74.005608       |40.706551       |CRD         |3.5        |0.5      |0.5    |0.8       |0.0         |5.3  |2013-01-01 00:01:00|1  |\n",
      "|E0F370CA508B7E8F639D7FAB3798F399|5AE4F2FB8B90CA06F9AA39FEF82DE2BB|2013-01-01 00:01:35|2013-01-01 00:02:23|48               |1.2          |-73.977898      |40.757912      |-73.97802        |40.757362       |DIS         |2.5        |0.5      |0.5    |0.0       |0.0         |3.5  |2013-01-01 00:01:35|1  |\n",
      "|08E54F4C460720DDE43460E354486FBC|33276CA24A915CBD668AF96873D07883|2013-01-01 00:00:00|2013-01-01 00:02:44|163              |0.5          |-73.999878      |40.743343      |-74.003708       |40.74828        |CRD         |4.0        |0.5      |0.5    |1.0       |0.0         |6.0  |2013-01-01 00:00:00|1  |\n",
      "|0CEBE42EAF42C338052FAE8D61612C58|CC7A4176549BA819E07D4A0463F87D2E|2013-01-01 00:00:00|2013-01-01 00:03:00|180              |1.56         |-74.00975       |40.706432      |-73.971985       |40.794716       |CSH         |6.5        |0.5      |0.5    |0.0       |0.0         |7.5  |2013-01-01 00:00:00|1  |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+-----+-------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now create a new session\n",
    "new_spark = SparkSession.builder.appName(\"CheckParquet\").getOrCreate()\n",
    "\n",
    "df_check = new_spark.read.parquet(\"data/kafka_cleaned_partitioned/year=2013/month=1/part-00000-b700166a-b17a-4a71-8686-e17a7e547570.c000.snappy.parquet\")\n",
    "df_check.show(10, truncate=False)\n",
    "\n",
    "new_spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
