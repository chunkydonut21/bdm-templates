{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark kafka-python delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth, lit, udf, floor, concat, lit, window, desc, expr, current_timestamp\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\"project2_debs_grand_challenge\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"medallion\", \"hack_license\", \"pickup_datetime\", \"dropoff_datetime\",\n",
    "    \"trip_time_in_secs\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "    \"dropoff_longitude\", \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
    "    \"surcharge\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"total_amount\"\n",
    "]\n",
    "\n",
    "\n",
    "df = spark.read.option(\"header\", \"false\").csv(\"data/sorted_data.csv\")\n",
    "df = df.toDF(*columns)\n",
    "df.printSchema()\n",
    "\n",
    "#Take 1GB of original data\n",
    "df_sample = df.sample(withReplacement=False, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#converting pickup and dropoff datetimes to to_timestamp\n",
    "df_sample = df_sample.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                     .withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "#Removing null or 0.0 columns and unknown licenses or drivers\n",
    "df_clean = df_sample.filter(\n",
    "    (col(\"medallion\").isNotNull()) & (col(\"medallion\") != \"0\") & (col(\"medallion\") != \"UNKNOWN\") &\n",
    "    (col(\"hack_license\").isNotNull()) & (col(\"hack_license\") != \"0\") & (col(\"hack_license\") != \"UNKNOWN\") &\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &\n",
    "    (col(\"trip_time_in_secs\").isNotNull()) & (col(\"trip_time_in_secs\") != 0) &\n",
    "    (col(\"trip_distance\").isNotNull()) & (col(\"trip_distance\") != 0) &\n",
    "    (col(\"pickup_longitude\").isNotNull()) & (col(\"pickup_longitude\") != 0.0) &\n",
    "    (col(\"pickup_latitude\").isNotNull()) & (col(\"pickup_latitude\") != 0.0) &\n",
    "    (col(\"dropoff_longitude\").isNotNull()) & (col(\"dropoff_longitude\") != 0.0) &\n",
    "    (col(\"dropoff_latitude\").isNotNull()) & (col(\"dropoff_latitude\") != 0.0) &\n",
    "    (col(\"trip_distance\").cast(\"float\") > 0) &\n",
    "    (col(\"fare_amount\").cast(\"float\") > 0)\n",
    ")\n",
    "\n",
    "# Convert relevant columns to appropriate data types for numerical computations\n",
    "df_clean = df_clean.withColumn(\"trip_time_in_secs\", col(\"trip_time_in_secs\").cast(\"int\")) \\\n",
    "                   .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"float\")) \\\n",
    "                   .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"surcharge\", col(\"surcharge\").cast(\"float\")) \\\n",
    "                   .withColumn(\"mta_tax\", col(\"mta_tax\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(\"float\"))\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build the Time Model\n",
    "# Convert the pickup datetime to extract year, month, and day.\n",
    "df_clean = df_clean.withColumn(\"pickup_year\", year(\"pickup_datetime\")) \\\n",
    "                   .withColumn(\"pickup_month\", month(\"pickup_datetime\")) \\\n",
    "                   .withColumn(\"pickup_day\", dayofmonth(\"pickup_datetime\"))\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Write Cleansed Data with File Partitioning (Parquet format)\n",
    "output_path = \"output/cleansed_taxi_data\"\n",
    "df_clean.write.partitionBy(\"pickup_year\", \"pickup_month\", \"pickup_day\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Compute the maximum dropoff datetime in the dataset\n",
    "max_dropoff = df_clean.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "print(\"Max dropoff datetime:\", max_dropoff)\n",
    "\n",
    "# 2. Define a reference time (30 minutes before max_dropoff)\n",
    "ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "# 3. Define grid cell size (here, 0.01 degrees is used as an approximation)\n",
    "cell_size = 0.01\n",
    "\n",
    "# 4. Create grid cell identifiers for pickup (start_cell) and drop-off (end_cell)\n",
    "df_routes = df_clean.withColumn(\n",
    "    \"start_cell\",\n",
    "    concat(\n",
    "        floor(col(\"pickup_latitude\") / lit(cell_size)),\n",
    "        lit(\"_\"),\n",
    "        floor(col(\"pickup_longitude\") / lit(cell_size))\n",
    "    )\n",
    ").withColumn(\n",
    "    \"end_cell\",\n",
    "    concat(\n",
    "        floor(col(\"dropoff_latitude\") / lit(cell_size)),\n",
    "        lit(\"_\"),\n",
    "        floor(col(\"dropoff_longitude\") / lit(cell_size))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Filter the DataFrame to include only trips whose dropoff time is within the last 30 minutes\n",
    "df_last30 = df_routes.filter(col(\"dropoff_datetime\") >= lit(ref_time))\n",
    "\n",
    "# 6. Group by start and end cells and count the number of rides,\n",
    "#    then rename the count column for clarity.\n",
    "df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "\n",
    "# 7. Order the routes by descending ride counts and take the top 10\n",
    "top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "\n",
    "# 8. Show the results\n",
    "top10_routes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2:  Query results must be updated whenever any of the 10 most frequent routes change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Writing the cleansed data to a Delta table in a writable directory.\n",
    "df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/taxi_data\")\n",
    "df_stream = spark.readStream.format(\"delta\").load(\"/tmp/delta/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Read the Delta table in batch mode\n",
    "check_df = spark.read.format(\"delta\").load(\"/tmp/delta/taxi_data\")\n",
    "check_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define your grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "# This is your foreachBatch function to process each micro-batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Compute the 30-minute window based on the batchâ€™s max dropoff\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "    ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # Compute grid cell IDs for pickup and dropoff using the 500m grid\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"pickup_cell_east\", floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"pickup_cell_south\", floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"start_cell\", concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"dropoff_cell_east\", floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell_south\", floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"end_cell\", concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # Filter out trips that are out-of-bounds (only consider cells 1 to 300)\n",
    "    batch_df = batch_df.filter(\n",
    "        (col(\"pickup_cell_east\").between(1, 300)) &\n",
    "        (col(\"pickup_cell_south\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_east\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_south\").between(1, 300))\n",
    "    )\n",
    "\n",
    "    # Filter for trips with dropoff_datetime >= ref_time (last 30 minutes)\n",
    "    df_last30 = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time))\n",
    "    print(f\"Window filter: dropoff_datetime >= {ref_time}\")\n",
    "    print(\"df_last30 count =\", df_last30.count())\n",
    "    df_last30.show(5)\n",
    "\n",
    "    # Aggregate routes and get top 10 most frequent\n",
    "    df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "    top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "    top10_list = top10_routes.collect()\n",
    "\n",
    "    # Determine a triggering event and compute delay\n",
    "    # Choose the event with the maximum dropoff_datetime as the trigger\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "\n",
    "    # Build the output row\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            route = top10_list[i]\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = route[\"start_cell\"]\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = route[\"end_cell\"]\n",
    "        else:\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id} :\", output_row)\n",
    "    \n",
    "    # Define the output schema explicitly\n",
    "    output_schema = StructType([\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"start_cell_id_1\", StringType(), True),\n",
    "        StructField(\"end_cell_id_1\", StringType(), True),\n",
    "        StructField(\"start_cell_id_2\", StringType(), True),\n",
    "        StructField(\"end_cell_id_2\", StringType(), True),\n",
    "        StructField(\"start_cell_id_3\", StringType(), True),\n",
    "        StructField(\"end_cell_id_3\", StringType(), True),\n",
    "        StructField(\"start_cell_id_4\", StringType(), True),\n",
    "        StructField(\"end_cell_id_4\", StringType(), True),\n",
    "        StructField(\"start_cell_id_5\", StringType(), True),\n",
    "        StructField(\"end_cell_id_5\", StringType(), True),\n",
    "        StructField(\"start_cell_id_6\", StringType(), True),\n",
    "        StructField(\"end_cell_id_6\", StringType(), True),\n",
    "        StructField(\"start_cell_id_7\", StringType(), True),\n",
    "        StructField(\"end_cell_id_7\", StringType(), True),\n",
    "        StructField(\"start_cell_id_8\", StringType(), True),\n",
    "        StructField(\"end_cell_id_8\", StringType(), True),\n",
    "        StructField(\"start_cell_id_9\", StringType(), True),\n",
    "        StructField(\"end_cell_id_9\", StringType(), True),\n",
    "        StructField(\"start_cell_id_10\", StringType(), True),\n",
    "        StructField(\"end_cell_id_10\", StringType(), True),\n",
    "        StructField(\"delay\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create the result DataFrame using the explicit schema\n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    \n",
    "    # Write the result_df as a table (it will create the table if it doesn't exist)\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"frequent_routes\")\n",
    "    \n",
    "    # Optional: Show the result DataFrame\n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# Ensure that your streaming DataFrame has proper types and includes an ingest_time column\n",
    "df_stream = df_stream.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_stream = df_stream.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_stream = df_stream.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Use trigger(once=True) to process existing data exactly one time\n",
    "query = (\n",
    "    df_stream.writeStream\n",
    "    .trigger(once=True)  #This means the query to run just once\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Display the result\n",
    "spark.sql(\"SELECT * FROM frequent_routes\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Profitable Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Report only the 10 most profitable areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "def process_batch_query2(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(f\"Batch {batch_id}: Empty batch.\")\n",
    "        return\n",
    "\n",
    "    # Compute reference times based on the batchâ€™s maximum dropoff_datetime\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        print(f\"Batch {batch_id}: max_dropoff is None.\")\n",
    "        return\n",
    "    # For profit: consider trips ending in the last 15 minutes\n",
    "    ref_time_profit = max_dropoff - timedelta(minutes=15)\n",
    "    # For empty taxis: consider taxis whose last dropoff was within the last 30 minutes\n",
    "    ref_time_empty = max_dropoff - timedelta(minutes=30)\n",
    "    print(f\"Batch {batch_id}: ref_time_profit = {ref_time_profit}, ref_time_empty = {ref_time_empty}\")\n",
    "\n",
    "    # Compute profit aggregate per area (using pickup location)\n",
    "    profit_df = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_profit)) \\\n",
    "        .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "        .withColumn(\n",
    "            \"pickup_cell_east\",\n",
    "            floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell\",\n",
    "            concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    profit_agg = profit_df.groupBy(\"pickup_cell\") \\\n",
    "        .agg(F.expr(\"approx_percentile(profit, 0.5) as median_profit\"))\n",
    "    \n",
    "    # Compute empty taxi aggregate per area (using dropoff location)\n",
    "    w = Window.partitionBy(\"medallion\").orderBy(col(\"dropoff_datetime\").desc())\n",
    "    last_dropoff_df = batch_df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "        .filter(col(\"rn\") == 1)\n",
    "    empty_df = last_dropoff_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_empty)) \\\n",
    "        .withColumn(\n",
    "            \"dropoff_cell_east\",\n",
    "            floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell\",\n",
    "            concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    empty_agg = empty_df.groupBy(\"dropoff_cell\") \\\n",
    "        .agg(F.countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "    \n",
    "    # Join the two aggregates on the cell identifier.\n",
    "    area_df = profit_agg.join(empty_agg, profit_agg.pickup_cell == empty_agg.dropoff_cell, \"inner\") \\\n",
    "        .select(profit_agg.pickup_cell.alias(\"cell_id\"), \"median_profit\", \"empty_taxis\") \\\n",
    "        .filter(col(\"empty_taxis\") > 0) \\\n",
    "        .withColumn(\"profitability\", col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "    \n",
    "    top10_areas = area_df.orderBy(col(\"profitability\").desc()).limit(10)\n",
    "    top10_list = top10_areas.collect()\n",
    "    \n",
    "    # Determine a triggering event and compute processing delay.\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    # Use asDict() to safely check for \"ingest_time\"\n",
    "    trigger_row_dict = trigger_row.asDict()\n",
    "    if \"ingest_time\" in trigger_row_dict:\n",
    "        ingest_time = trigger_row_dict[\"ingest_time\"]\n",
    "    else:\n",
    "        ingest_time = trigger_dropoff  # fallback if missing\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "    \n",
    "    # Build the output row.\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            area = top10_list[i]\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = area[\"cell_id\"]\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = str(area[\"empty_taxis\"])\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = area[\"median_profit\"]\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = area[\"profitability\"]\n",
    "        else:\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id}:\", output_row)\n",
    "    \n",
    "    fields = [\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        fields.extend([\n",
    "            StructField(f\"profitable_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"empty_taxies_in_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"median_profit_in_cell_id_{i+1}\", DoubleType(), True),\n",
    "            StructField(f\"profitability_of_cell_{i+1}\", DoubleType(), True)\n",
    "        ])\n",
    "    fields.append(StructField(\"delay\", DoubleType(), True))\n",
    "    output_schema = StructType(fields)\n",
    "    \n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    # Write the result_df as a table\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"most_profitable_areas_result\")\n",
    "    \n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# Ensure your streaming DataFrame has an ingest_time column.\n",
    "df_stream = df_stream.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "query2 = (\n",
    "    df_stream.writeStream\n",
    "    .trigger(once=True) \n",
    "    .foreachBatch(process_batch_query2)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query2.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM most_profitable_areas_result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2: Resulting stream of the query provide the 10 most profitable areas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Grid Constants for 250m x 250m grid\n",
    "# The gridâ€™s center of cell 1.1 remains at (41.474937, -74.913585).\n",
    "# For 250m resolution, we use half the previous deltas:\n",
    "new_delta_lat = 0.0045 / 2    # â‰ˆ0.00225\n",
    "new_delta_lon = 0.0060 / 2    # â‰ˆ0.0030\n",
    "\n",
    "# Define the foreachBatch function for Query 2 Part 2\n",
    "def process_batch_query2_part2(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        print(f\"Batch {batch_id}: Empty batch.\")\n",
    "        return\n",
    "\n",
    "    # Compute reference times using batchâ€™s maximum dropoff_datetime.\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        print(f\"Batch {batch_id}: max_dropoff is None.\")\n",
    "        return\n",
    "    # For profit computation, consider trips that ended in the last 15 minutes.\n",
    "    ref_time_profit = max_dropoff - timedelta(minutes=15)\n",
    "    # For empty taxis, consider taxis whose last dropoff was within the last 30 minutes.\n",
    "    ref_time_empty = max_dropoff - timedelta(minutes=30)\n",
    "    print(f\"Batch {batch_id}: ref_time_profit = {ref_time_profit}, ref_time_empty = {ref_time_empty}\")\n",
    "\n",
    "    # Compute profit aggregate per area (using pickup location).\n",
    "    # Only consider trips with dropoff_datetime >= ref_time_profit.\n",
    "    profit_df = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_profit)) \\\n",
    "        .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "        .withColumn(\n",
    "            \"pickup_cell_east\",\n",
    "            floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(new_delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(new_delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"pickup_cell\",\n",
    "            concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    profit_agg = profit_df.groupBy(\"pickup_cell\") \\\n",
    "        .agg(F.expr(\"approx_percentile(profit, 0.5) as median_profit\"))\n",
    "    \n",
    "    # Compute empty taxi aggregate per area (using dropoff location).\n",
    "    # For each taxi (medallion), take the latest dropoff event.\n",
    "    w = Window.partitionBy(\"medallion\").orderBy(col(\"dropoff_datetime\").desc())\n",
    "    last_dropoff_df = batch_df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "        .filter(col(\"rn\") == 1)\n",
    "    empty_df = last_dropoff_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_empty)) \\\n",
    "        .withColumn(\n",
    "            \"dropoff_cell_east\",\n",
    "            floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(new_delta_lon)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell_south\",\n",
    "            floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(new_delta_lat)) + 1\n",
    "        ).withColumn(\n",
    "            \"dropoff_cell\",\n",
    "            concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "        )\n",
    "    empty_agg = empty_df.groupBy(\"dropoff_cell\") \\\n",
    "        .agg(F.countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "    \n",
    "    # Join the aggregates on the cell identifier.\n",
    "    # (We assume the area is defined by the same grid cell for pickup and dropoff.)\n",
    "    area_df = profit_agg.join(empty_agg, profit_agg.pickup_cell == empty_agg.dropoff_cell, \"inner\") \\\n",
    "        .select(profit_agg.pickup_cell.alias(\"cell_id\"), \"median_profit\", \"empty_taxis\") \\\n",
    "        .filter(col(\"empty_taxis\") > 0) \\\n",
    "        .withColumn(\"profitability\", col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "    \n",
    "    # Get the top 10 areas by profitability.\n",
    "    top10_areas = area_df.orderBy(col(\"profitability\").desc()).limit(10)\n",
    "    top10_list = top10_areas.collect()\n",
    "    \n",
    "    # Determine a triggering event and compute processing delay.\n",
    "    # Choose the event with maximum dropoff_datetime as the trigger.\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    # Convert the row to a dictionary for safe field access.\n",
    "    trigger_row_dict = trigger_row.asDict()\n",
    "    # Use ingest_time if available; otherwise, use trigger_dropoff as fallback.\n",
    "    ingest_time = trigger_row_dict[\"ingest_time\"] if \"ingest_time\" in trigger_row_dict else trigger_dropoff\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "    \n",
    "    # Build the output row.\n",
    "    # The required output columns are:\n",
    "    # pickup_datetime, dropoff_datetime, then for each of the 10 areas:\n",
    "    # profitable_cell_id_i, empty_taxies_in_cell_id_i, median_profit_in_cell_id_i, profitability_of_cell_i, and finally delay.\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            area = top10_list[i]\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = area[\"cell_id\"]\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = area[\"empty_taxis\"]  # as integer\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = area[\"median_profit\"]\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = area[\"profitability\"]\n",
    "        else:\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id}:\", output_row)\n",
    "    \n",
    "    # Define the output schema.\n",
    "    out_fields = [\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        out_fields.extend([\n",
    "            StructField(f\"profitable_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"empty_taxies_in_cell_id_{i+1}\", IntegerType(), True),\n",
    "            StructField(f\"median_profit_in_cell_id_{i+1}\", DoubleType(), True),\n",
    "            StructField(f\"profitability_of_cell_{i+1}\", DoubleType(), True)\n",
    "        ])\n",
    "    out_fields.append(StructField(\"delay\", DoubleType(), True))\n",
    "    output_schema = StructType(out_fields)\n",
    "    \n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    # Write the result_df as a table\n",
    "    result_df.write.mode(\"append\").saveAsTable(\"profitable_areas_streaming_result\")\n",
    "    \n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# Ensure your streaming DataFrame (streaming_df) has an ingest_time column.\n",
    "df_stream = df_stream.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Set up the streaming query using foreachBatch.\n",
    "query2_part2 = (\n",
    "    df_stream.writeStream\n",
    "    .trigger(once=True) \n",
    "    .foreachBatch(process_batch_query2_part2)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query2_part2.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM profitable_areas_streaming_result\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
